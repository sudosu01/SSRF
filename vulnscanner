import requests, re, threading
from urllib.parse import urlparse
from concurrent.futures import ThreadPoolExecutor
from tqdm import tqdm
import sys
import os

WAYBACK_API = "http://web.archive.org/cdx/search/cdx"

headers = {
    "User-Agent": "Mozilla/5.0 (compatible; vulnscanner/1.0; +https://example.com/bot)"
}

sensitive_patterns = {
    "Password": r'(?i)password[\'"=:\s]+([^\s\'",]+)',
    "API Key": r'(?i)(api[_-]?key|access[_-]?token|secret|auth[_-]?token)[\'"=:\s]+([a-z0-9-_]{16,})',
    "JWT": r'eyJ[a-zA-Z0-9-_]+\.[a-zA-Z0-9-_]+\.[a-zA-Z0-9-_]+',
    "Email": r'[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+',
    "Phone": r'\+?\d[\d\s\-\(\)]{8,}\d',
    "Cookie": r'(?i)Set-Cookie:\s*([^;]+)'
}

interesting_endpoints = [
    'login', 'logout', 'signup', 'admin', 'dashboard', 'user', 'account', 'subscribe', 'unsubscribe', 'redirect',
    'success', 'callback', 'confirm', 'auth', 'token', 'session', 'update', 'change'
]

xss_patterns = [
    r'(?i)<script[^>]*>.*?</script>',
    r'(?i)on\w+=["\'].*?["\']',
    r'(?i)javascript:',
    r'(?i)<img[^>]+onerror=',
    r'(?i)<svg[^>]+onload=',
]

def fetch_wayback_urls(domain):
    try:
        params = {
            "url": f"{domain}/*",
            "output": "json",
            "collapse": "urlkey",
            "filter": "statuscode:200",
            "limit": "5000"
        }
        res = requests.get(WAYBACK_API, params=params, headers=headers, timeout=20)
        if res.status_code == 200:
            urls = list({row[2] for row in res.json()[1:]})
            print(f"  Found {len(urls)} URLs from Wayback")
            return urls
    except Exception as e:
        print(f"[!] Error fetching Wayback URLs for {domain}: {e}")
    return []

def extract_js_urls(urls):
    return [url for url in urls if url.endswith(".js")]

def scan_url(url):
    findings = []
    try:
        r = requests.get(url, headers=headers, timeout=10)
        content = r.text

        for key, pattern in sensitive_patterns.items():
            for match in re.finditer(pattern, content):
                snippet = match.group(0)
                findings.append((key, snippet.strip(), url))

        for pattern in xss_patterns:
            for match in re.finditer(pattern, content):
                findings.append(("XSS Pattern", match.group(0).strip(), url))

        parsed = urlparse(url)
        if any(x in url.lower() for x in interesting_endpoints):
            findings.append(("Interesting Endpoint", parsed.path, url))

        if re.search(r'/\d{1,10}(/|$|\?)', url):
            findings.append(("Potential IDOR", url.split('?')[0], url))

    except Exception:
        pass

    return findings

def load_targets():
    user_input = input("Enter comma-separated URLs or a filename: ").strip()
    if os.path.isfile(user_input):
        with open(user_input) as f:
            return [line.strip() for line in f if line.strip()]
    return [u.strip() for u in user_input.split(",") if u.strip()]

def main():
    targets = load_targets()
    all_urls = []
    print(f"[*] Fetching archived URLs from Wayback for domain: {targets[0]}")
    for domain in targets:
        urls = fetch_wayback_urls(domain)
        all_urls.extend(urls)

    js_urls = extract_js_urls(all_urls)
    print(f"  Found {len(js_urls)} JS files to scan for sensitive info")

    print(f"[*] Starting scan on {len(all_urls)} URLs with 30 threads...\n")
    results = []

    with ThreadPoolExecutor(max_workers=30) as executor:
        for res in tqdm(executor.map(scan_url, all_urls), total=len(all_urls)):
            results.extend(res)

    print("\nScan Complete. Results:\n")

    if not results:
        print("No issues found.")
        return

    grouped = {}
    for issue_type, snippet, url in results:
        grouped.setdefault(issue_type, []).append((snippet, url))

    for issue, entries in grouped.items():
        print(f"\n[+] {issue} ({len(entries)} findings)")
        for snippet, url in entries:
            print(f"  - {url}\n    â†’ \033[1m{snippet}\033[0m")

if __name__ == "__main__":
    main()
